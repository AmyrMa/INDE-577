{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d97593",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "The idead of ensemble methods is use the idea of the wisdom of crowd. We can use multiple model to predict the result, and then pick the majority votes for the classfication or the averge value for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2f0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66685f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/AmyrMa/INDE-577/main/data/data.csv')\n",
    "df = df.drop(['id','Unnamed: 32'], axis = 1)\n",
    "df['diagnosis'] = np.where(df['diagnosis'] == 'M', 1, 0)\n",
    "y = np.array(df['diagnosis'])\n",
    "X = df.drop(['diagnosis'], axis = 1)\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.25, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5055ffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stump Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94        98\n",
      "           1       0.89      0.87      0.88        45\n",
      "\n",
      "    accuracy                           0.92       143\n",
      "   macro avg       0.91      0.91      0.91       143\n",
      "weighted avg       0.92      0.92      0.92       143\n",
      " \n",
      "\n",
      "Tree Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95        98\n",
      "           1       0.89      0.91      0.90        45\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.93      0.93      0.93       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      " \n",
      "\n",
      "Bagging Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97        98\n",
      "           1       0.93      0.96      0.95        45\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.96      0.96      0.96       143\n",
      "weighted avg       0.97      0.97      0.97       143\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stump_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "stump_clf.fit(X_train, y_train)\n",
    "stump_y_pred = stump_clf.predict(X_test)\n",
    "print(f\"Stump Classification Report\")\n",
    "print(classification_report(y_test, stump_y_pred), \"\\n\")\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=9, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_pred = tree_clf.predict(X_test)\n",
    "print(f\"Tree Classification Report\")\n",
    "print(classification_report(y_test, tree_y_pred), \"\\n\")\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "                            n_estimators = 500,\n",
    "                            bootstrap = True,\n",
    "                            n_jobs = -1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_y_pred = bag_clf.predict(X_test)\n",
    "print(f\"Bagging Classification Report\")\n",
    "print(classification_report(y_test, bag_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d704a89d",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99b6e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97        98\n",
      "           1       0.93      0.96      0.95        45\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.96      0.96      0.96       143\n",
      "weighted avg       0.97      0.97      0.97       143\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_clf = RandomForestClassifier(max_depth = 1, n_estimators = 500,\n",
    "                                    bootstrap = True,\n",
    "                                    n_jobs = -1)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_y_pred = bag_clf.predict(X_test)\n",
    "print(f\"Forest Classification Report\")\n",
    "print(classification_report(y_test, forest_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a99dc8",
   "metadata": {},
   "source": [
    "In class we learned that the precision of the random forest are same as the bagging classfier. whichi is also true for this data set. Boosting is an ensemble method that trains predictors sequentially, each trying to correct the errors made by the previous predictor. In this lecture we consider two well known boosting methods, namely AdaBoost and gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8cfa9",
   "metadata": {},
   "source": [
    "# Ada Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9462bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96        98\n",
      "           1       0.91      0.93      0.92        45\n",
      "\n",
      "    accuracy                           0.95       143\n",
      "   macro avg       0.94      0.95      0.94       143\n",
      "weighted avg       0.95      0.95      0.95       143\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1, random_state=42), \n",
    "                             n_estimators = 10,\n",
    "                             algorithm = \"SAMME.R\",\n",
    "                             learning_rate = 0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_y_pred = ada_clf.predict(X_test)\n",
    "print(f\"AdaBoost Classification Report\")\n",
    "print(classification_report(y_test, ada_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b0feae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a4af9",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "Another popular boosting method is gradient boosting. This method works by sequentially adding predictors to an ensemble, each correcting is predecessor. The difference between this method and AdaBoost is that gradient boosting tries to fit the new predictor to the residual errors made by the previous predictor. Recall the residual error denoted by error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276ca0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.2, max_depth=2, n_estimators=150,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb_reg = GradientBoostingRegressor(max_depth = 2, n_estimators = 150, learning_rate=0.2, random_state=42)\n",
    "gb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99b1074d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97        98\n",
      "           1       0.93      0.96      0.95        45\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.96      0.96      0.96       143\n",
      "weighted avg       0.97      0.97      0.97       143\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = gb_reg.predict(X_test)\n",
    "y_pred = np.where(y_pred >0.5, 1,0)\n",
    "print(f\"Gradient Boosting Classification Report\")\n",
    "print(classification_report(y_test,y_pred), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ba26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
